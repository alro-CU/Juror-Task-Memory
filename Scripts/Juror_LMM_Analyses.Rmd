---
output:
  html_document: default
  word_document: default
---
## Loading in Data 

```{r ,setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
#loading dependencies and data
library(rethinking)
library(tidyverse)
library(dplyr)
library(lme4)
library(lmerTest)
library(sjPlot)
library(yhat)
library(lmerTest)
library(car)

library(sjPlot)
library(sjmisc)
library(sjlabelled)
library(partR2)
library(rstanarm)
library(modelsummary)
library(MuMIn)

bard_df <-read.csv("bard_ratings.csv")
bard_df <- bard_df %>%
  mutate(bard = ifelse(bardguilt == TRUE, 1, 0))


#cleaning data
bard_z <- bard_df %>%
  mutate(z_episodic = ((recall-mean(recall))/sd(recall)),
         z_semantic = ((know-mean(know))/sd(know)),
         z_realism = ((realrate-mean(realrate))/sd(realrate)),
         z_case_strength = ((rating-mean(rating))/sd(rating)))
```

# Run pairwise regressions while including random intercepts and random slopes

```{r}
#episodic predicts case strength  
rg_e <-lmer(z_case_strength ~ z_episodic + (1 + z_episodic |num_uid) + (1|scenario), data=bard_z)
summary(rg_e)
tab_model(rg_e, pred.labels = c("Episodic Memory"), dv.labels = c("Case Strength Rating"))
#b=0.34, CI = [0.28-0.40], p < 0.001

#semantic predicts case strength
rg_sem <-lmer(z_case_strength ~ z_semantic + (1 + z_semantic |num_uid) + (1|scenario), data=bard_z)
summary(rg_sem)
tab_model(rg_sem, pred.labels = c("Semantic Memory"), dv.labels = c("Case Strength Rating"))
#b=0.30, CI = [0.24-0.37], p < 0.001

#realism predicts case strength
rg_real <-lmer(z_case_strength ~ z_realism + (1 + z_realism |num_uid) + (1|scenario), data=bard_z)
summary(rg_real)
tab_model(rg_real, pred.labels = c("Perceived Realism"), dv.labels = c("Case Strength Rating"))
#b= 0.37, CI = [0.31-0.44], p < 0.001



#episodic predicts semantic 
rg_e_sem <-lmer(z_semantic ~ z_episodic + (1 + z_episodic |num_uid) + (1|scenario), data=bard_z)
summary(rg_e_sem)
tab_model(rg_e_sem, pred.labels = c("Episodic Memory"), dv.labels = c("Semantic Memory"))
#b= 0.57, CI = [0.51-0.63], p < 0.001

#semantic predicts realism 
rg_sem_real <-lmer(z_realism ~ z_semantic + (1 + z_semantic |num_uid) + (1|scenario), data=bard_z)
summary(rg_sem_real)
tab_model(rg_sem_real, pred.labels = c("Semantic Memory"), dv.labels = c("Perceived Realism"))
#b= 0.40, CI = [0.33-0.48], p < 0.001

#episodic predicts realism 
rg_e_real <-lmer(z_realism ~ z_episodic + (1 + z_episodic |num_uid) + (1|scenario), data=bard_z)
summary(rg_e_real)
tab_model(rg_e_real, pred.labels = c("Episodic Memory"), dv.labels = c("Perceived Realism"))
#b=0.38, CI = [0.31-0.44], p < 0.001
```

# Linear Mixed Effects Model including random intercepts AND random slopes

Here, we use the z scored measures of memory and realism to predict z scored case strength rating, while estimating random intercepts for subjects and scenarios as well as estimating random slopes for episodic memory, semantic memory, and realism. 


```{r}
#run lmm using z scored ratings - include random slopes 
rg_new <-lmer(z_case_strength ~ z_episodic + z_semantic + z_realism + (1 + z_episodic + z_semantic + z_realism |num_uid) + (1|scenario), data=bard_z)
summary(rg_new)


#library(sjPlot)
tab_model(rg_new, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism"), dv.labels = c("Case Strength Rating"))
```

```{r}
#run lmm using z scored ratings - include random slopes - drop correlations between slopes
rg_2 <-lmer(z_case_strength ~ z_episodic + z_semantic + z_realism + (1 + z_episodic + z_semantic + z_realism || num_uid) + (1|scenario), data=bard_z)
summary(rg_2)

#view model results
tab_model(rg_2, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism"), dv.labels = c("Case Strength Rating"))

#view r2 of model 
r.squaredGLMM(rg_2)
```
## variance partitioning 

```{r}
#partitioning variance on rg_2
part_var_z <- partR2(rg_2, partvars = c("z_episodic", "z_semantic", "z_realism"), nboot=100)

summary(part_var_z)
```


# Calculating shared and unique variance components based on PartR2 output 

```{r}
total_var = 0.1787
episodic_memory <- 0.0363
semantic_memory <- 0.0108
p_realism <- 0.1069

episodic_semantic <- 0.0715	
episodic_realism <- 0.1481
semantic_realism <- 0.1254


#shared variance between episodic and semantic 
shared_e_s <- episodic_semantic - episodic_memory - semantic_memory
print(shared_e_s)

# 0.0244

#shared variance between episodic and realism
shared_e_r <- episodic_realism - episodic_memory - p_realism
print(shared_e_r)

# 0.0049

# shared variance between semantic and realism
shared_s_r <- semantic_realism - semantic_memory - p_realism
print(shared_s_r)

# 0.0077

#variance unique to episodic 
unique_ep <- episodic_memory - shared_e_r - shared_e_s 
print(unique_ep)
# 0.007

#variance unique to semantic
unique_s <- semantic_memory - shared_s_r - shared_e_s
print(unique_s)

# -0.0213, no unique variance from semantic

#variance unique to realism 
unique_r <- p_realism - shared_s_r - shared_e_r
print(unique_r)

#0.0943

# shared variance between all three predictors
common_all <- total_var - (unique_ep + unique_r + shared_e_r + shared_e_s + shared_s_r)
print(common_all)

#0.0404

#the summation of these values should add to total_var
common_all + shared_e_p + shared_e_s + shared_s_r + unique_ep + unique_r

```

#Multilevel Logistic regression 

Here, we predict BARD (guilty/not guilty) using the memory and realism measures as predictors. 

This model has the same random effects structure as the LMM predicting case strength (i.e. random intercepts for subject and scenario, random slopes for all predictors.)

# GLMER just memory effects on BARD

```{r}
#run glmer using z scored ratings - include random slopes - drop correlations between slopes
rg_glm <-glmer(bard ~ z_episodic + z_semantic + z_realism + (1 + z_episodic + z_semantic + z_realism || num_uid) + (1|scenario), data=bard_z, family = "binomial")
summary(rg_glm)

tab_model(rg_glm, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism"), dv.labels = c("BARD Decision"))
```

#GLM Mediation (includes case strength rating)

```{r}
#model including case strength as predictor
glm_full <- stan_glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1 + z_episodic + z_semantic + z_realism + z_case_strength || num_uid) + (1|scenario), data=bard_z, family = "binomial")

summary(glm_full)

#View model
tab_model(glm_full, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))

```

