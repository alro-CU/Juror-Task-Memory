---
output:
  word_document: default
  html_document: default
---
## Loading in Data 

```{r ,setup, include=FALSE}
knitr::opts_knit$set(root.dir = '/Users/rogershome/Desktop/Grad Stats 5741/Stats_Rethinking-Bayes/BDA_Final')
getwd()
```


```{r}
library(rethinking)
library(tidyverse)
#loading data and dependencies
library(dplyr)
library(lme4)
library(lmerTest)
library(sjPlot)
library(yhat)
library(lmerTest)
library(car)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
library(partR2)

bard_df <-read.csv("bard_ratings.csv")
bard_df <- bard_df %>%
  mutate(bard = ifelse(bardguilt == TRUE, 1, 0))
#View(bard_df)

#cleaning data
bard_z <- bard_df %>%
  mutate(z_episodic = ((recall-mean(recall))/sd(recall)),
         z_semantic = ((know-mean(know))/sd(know)),
         z_realism = ((realrate-mean(realrate))/sd(realrate)),
         z_case_strength = ((rating-mean(rating))/sd(rating)))
View(bard_z)

```

## Commonality Analysis / Variance Partitioning

```{r}

#regr <- lm(paragrap ~ general + sentence + wordc + wordm)

#View(df_corr)

#regr <-lm(rating ~ recall + know + realrate, data=df_corr)
#summary(regr)
#library(car)
#vif(regr)
#Common_coef<-commonalityCoefficients(df_corr, "rating", list("recall", "know", "realrate"), "F")
#print(Common_coef)

#View(df3)

#multilevel model using subj as random effect

#View(df2)
regr2 <-lmer(rating ~ recall + know + realrate + (1|num_uid) + (1|scenario), data=df2)
summary(regr2)
vif(regr2) #variance inflation 

regr_old <-lmer(rating ~ recall + know + realrate + (1|num_uid), data=df2)
summary(regr_old)
#making regression table
install.packages("sjlabelled")


tab_model(regr2, pred.labels = c("Intercept", "Episodic Recall", "Semantic Memory", "Perceived Realism"))


#RM2 marginal R squared: variance of fixed effects only
#R2c conditional R squared: variance accounted for by total model



#trying on our model 
results <- partR2(regr2, partvars = c("recall", "know", "realrate"), nboot=100)
print(results)
#summary(results)


```

# Running linear mixed effects models predicting case strength

```{r}
## running standard lmm with subjects as random effect

regr <-lmer(rating ~ recall + know + realrate + (1|num_uid), data=bard_z)
summary(regr)
vif(regr) #variance inflation 
tab_model(regr, pred.labels = c("Intercept", "Episodic Recall", "Semantic Memory", "Perceived Realism"))

#model two random effects - subj and scenario
regr3 <-lmer(rating ~ episodic + semantic + realism + (1|num_uid) + (1|scenario), data=bard_z)
summary(regr3)
tab_model(regr3, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism"), dv.labels = c("Case Strength Rating"))
```


# running lmm using zscores

```{r}

#run lmm using z scored ratings
regr_z_scores <-lmer(z_case_strength ~ z_episodic + z_semantic + z_realism + (1|num_uid) + (1|scenario), data=bard_z)
summary(regr_z_scores)
#library(sjPlot)
tab_model(regr_z_scores, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism"), dv.labels = c("Case Strength Rating"))

```


# Variance Partitioning on LMM

```{r}
#partitioning variance on regr_z_scores
part_var_z <- partR2(regr_z_scores, partvars = c("z_episodic", "z_semantic", "z_realism"), nboot=100)
summary(part_var_z)

```


#Multilevel Logistic regression 
Here, we will predict BARD (guilty/not guilty) using the memory and realism measures as predictors. We'll run the model using the glmer() function, with the family= binomial. 

This model has the same random effects structure as the LMM predicting case strength (i.e. random intercepts for subject and scenario.)


```{r}

#setting up multilevel logistic regression -random intercept for subjects
bard_regr <-glmer(bard ~ recall + know + realrate + (1|num_uid), data = bard_data, family = binomial)
summary(bard_regr)

#rescaling variables

#dividing all ratings by 100 
b_df <- bard_data %>%
  mutate(recall10 = recall/100,
          realrate10 = realrate/100,
          know10 = know/100,
         rating10 = rating/100,
         .keep = "all")

#View(b_df)

#running model with rescaled variables
b_regr <-glmer(bard ~ recall10 + know10 + realrate10 + (1|num_uid), data = b_df, family = binomial)
summary(b_regr)
tab_model(b_regr, pred.labels = c("Intercept", "Episodic Recall", "Semantic Memory", "Perceived Realism"))


#setting up multilevel logistic regression - 2 intercepts
l_regr <-glmer(bard ~ recall + know + realrate + (1|num_uid) + (1|scenario), data = bard_data, family = binomial)
summary(l_regr)
#running model with rescaled variables
b_regr2 <-glmer(bard ~ recall10 + know10 + realrate10 + (1|num_uid) + (1|scenario), data = b_df, family = binomial)
summary(b_regr2)
#viewing results 
tab_model(b_regr2, pred.labels = c("Intercept", "Episodic Recall", "Semantic Memory", "Perceived Realism"))

#run the variance partioning on the logistic regression 

```

```{r}
#running model with rescaled variables
b_regr2 <-glmer(bard ~ recall10 + know10 + realrate10 + (1|num_uid) + (1|scenario), data = b_df, family = binomial)
summary(b_regr2)
#viewing results 
tab_model(b_regr2, pred.labels = c("Intercept", "Episodic Recall", "Semantic Memory", "Perceived Realism"))


```



#histograms of all z scored ratings 

```{r}
hist(df_bard_z$z_episodic)

hist(df_bard_z$z_semantic)

hist(df_bard_z$z_realism)

hist(df_bard_z$z_case_strength)
```

# scatterplots of all ratings against case strength

```{r}

#episodic vs case strength
ggplot(df_bard_z, aes(x=z_episodic, y=z_case_strength)) + geom_point()
#semantic vs case strength
ggplot(df_bard_z, aes(x=z_semantic, y=z_case_strength)) + geom_point()
#realism vs case strength
ggplot(df_bard_z, aes(x=z_realism, y=z_case_strength)) + geom_point()
```

#pairwise models of all predictors

```{r}
#episodic predicts case strength
regr_ep <-lmer(z_case_strength ~ z_episodic + (1|num_uid) + (1|scenario), data=bard_z)
summary(regr_ep)
#beta of .30; p < 0.001

#semantic predicts case strength
regr_sem <-lmer(z_case_strength ~ z_semantic + (1|num_uid) + (1|scenario), data=bard_z)
summary(regr_sem)
#beta of .28; p < 0.001

#realism predicts case strength
regr_real <-lmer(z_case_strength ~ z_realism + (1|num_uid) + (1|scenario), data=bard_z)
summary(regr_real)
#beta of .35; p < 0.001

#episodic predicts semantic 
regr_ep_sem <-lmer(z_semantic ~ z_episodic + (1|num_uid) + (1|scenario), data=bard_z)
summary(regr_ep_sem)
#beta of .56; p < 0.001

#semantic predicts realism 
regr_real_sem <-lmer(z_realism ~ z_semantic + (1|num_uid) + (1|scenario), data=bard_z)
summary(regr_real_sem)
#beta of .36; p < 0.001

#episodic predicts realism 
regr_ep_real <-lmer(z_realism ~ z_episodic + (1|num_uid) + (1|scenario), data=bard_z)
summary(regr_ep_real)
#beta of .31; p < 0.001

```


# Rerunning the LLM with the z scored predictors

```{r}
##LLMmodel with optimizer 
regr_z_scores <-lmer(z_case_strength ~ z_episodic + z_semantic + z_realism + (1|num_uid) + (1|scenario), data=df_bard_z)
summary(regr_z_scores)

#viewing results
library(sjPlot)
tab_model(regr_z_scores, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism"), dv.labels = c("Case Strength Rating"))
```


#Rerunning GLM 

```{r}
##glm model with optimizer - z-scored predictors

b_zscored <-glmer(bard ~ z_episodic + z_semantic + z_realism + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
summary(b_zscored)
tab_model(b_zscored, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism"), dv.labels = c("BARD"))


```


#GLMER just case strength

```{r}
b_zscored_case <-glmer(bard ~ z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
summary(b_zscored_case)
tab_model(b_zscored_case, pred.labels = c("Intercept", "Case Strength"), dv.labels = c("BARD"))
```

#GLM Mediation (includes case strength rating)

```{r}
#running GLMER model using rstanarm

library(rstanarm)
library(rethinking)
#install.packages("modelsummary")
library(modelsummary)

bard_stan <- stan_glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial)

#viewing results
summary(bard_stan)

precis(bard_stan, prob = .95)

#alternative view
modelsummary(bard_stan, statistic = "conf.int")

#note which estimates have a confidence interval that includes zero 

#View model
tab_model(bard_stan, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))

```

# GLMER just memory effects on BARD

```{r}
#model
bard_memory_stan <- stan_glmer(bard ~ z_episodic + z_semantic + z_realism + (1|num_uid) + (1|scenario), data = bard_z, family = binomial)

#viewing results
summary(bard_memory_stan)

modelsummary(bard_memory_stan, statistic = "conf.int")

#sjplot
tab_model(bard_memory_stan, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism"), dv.labels = c("BARD"))
```

# Rerun LMER of memory effects on case strength 

This may require us to revisit the variance partitioning as well depending on the estimates. 


```{r}
#model
case_stan_mem <- stan_glmer(z_case_strength ~ z_episodic + z_semantic + z_realism + (1|num_uid) + (1|scenario), data = bard_z, family = gaussian)

summary(case_stan_mem)

#sjplot
tab_model(case_stan_mem, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism"), dv.labels = c("Case Strength"))
```

# Graphing 

```{r}
b_df$preds_prev = predict(b_opt, type="response")

plt_opt1 <- ggplot(b_df) + geom_smooth(aes(x=realrate10, y=preds_prev), color = "orange")  + 
  geom_point(aes(x=realrate10, y = bard), color = "darkorange") +
  xlab("Percieved Realism (points)") +
  ylab("Probability of Voting Guilty") 
plt_opt1

```

```{r}
plt_6 <- ggplot(b_df) + geom_smooth(aes(x=recall10, y=preds_prev), color = "darkgreen")  + 
  geom_point(aes(x=recall10, y = bard), color = "lightgreen") +
  xlab("Episodic Memory (points)") +
  ylab("Probability of Voting Guilty") 
plt_6
```
```{r}
plt_opt3 <- ggplot(b_df) + geom_smooth(aes(x=know10, y=preds_prev), color = "brown")  + 
  geom_point(aes(x=know10, y = bard), color = "maroon") +
  xlab("Semantic Memory (points)") +
  ylab("Probability of Voting Guilty") 
plt_opt3
```


```{r}
#variance partitioning on glmer 
library(partR2)
bard_var <- partR2(b_opt, partvars = c("recall10", "know10", "realrate10"), nboot=100)
print(bard_var)
```

```{r} 
#graphing 
glm_probs = data.frame(probs = predict(case_bard2, type="response"))
print(glm_probs)

glm_pred = glm_probs %>%
  mutate(pred = ifelse(probs>.5, "Guilty", "Not Guilty"))

b_df$bins <- cut(b_df$rating10, breaks=c(0,.20,.40,.60, .80, 1), labels=c("0","20","40", "60", "80"))

b_df$bin_val <-as.numeric(b_df$bins)


library(ggplot2)

ggplot(b_df, aes(x=rating10, y=bard)) + geom_point() +
      stat_smooth(method="glm", color="lightblue", se=FALSE, 
                method.args = list(family=binomial))


#### graphing from Pearson 2018 paper

preds <- predict.glm(case_bard2, newdata=data.frame(rating=1:100), type="response")
View(preds)
pred_dat <- data.frame(prob=preds, rating=1:100)
pred_dat <- data.frame(prob=preds, rating=1:100)


plt_4 <- ggplot(b_df) + geom_smooth(aes(x=rating10, y=pred), color = "slateblue")  + 
  geom_point(aes(x=rating10, y = bard), color = "plum3") +
  xlab("Case Strength (points)") +
  ylab("Probability of Voting Guilty") 
plt_4


#curves for each of the memory measures as well 

plt_4 <- ggplot(b_df) + geom_smooth(aes(x=rating10, y=pred), color = "slateblue")  + 
  geom_point(aes(x=rating10, y = bard), color = "plum3") +
  xlab("Case Strength (points)") +
  ylab("Probability of Voting Guilty") 
plt_4

```


```{r}
library(ggplot2)
plt_5 <- ggplot(b_df) + geom_smooth(aes(x=realrate10, y=pred), color = "orange")  + 
  geom_point(aes(x=realrate10, y = bard), color = "darkorange") +
  xlab("Percieved Realism (points)") +
  ylab("Probability of Voting Guilty") 
plt_5
```

```{r}
plt_6 <- ggplot(b_df) + geom_smooth(aes(x=recall10, y=pred), color = "darkgreen")  + 
  geom_point(aes(x=recall10, y = bard), color = "lightgreen") +
  xlab("Episodic Memory (points)") +
  ylab("Probability of Voting Guilty") 
plt_6
```

```{r}
plt_7 <- ggplot(b_df) + geom_smooth(aes(x=know10, y=pred), color = "brown")  + 
  geom_point(aes(x=know10, y = bard), color = "maroon") +
  xlab("Semantic Memory (points)") +
  ylab("Probability of Voting Guilty") 
plt_7
```



