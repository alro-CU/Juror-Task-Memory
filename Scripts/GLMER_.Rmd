---
title: "GLMER_Troubleshoot"
author: "Alex Rogers"
date: "2024-07-23"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#loading data and dependencies
library(dplyr)
library(lme4)
library(lmerTest)
library(sjPlot)
bard_df <-read.csv("bard_ratings.csv")
bard_df <- bard_df %>%
  mutate(bard = ifelse(bardguilt == TRUE, 1, 0))
#View(bard_df)

#cleaning data
bard_z <- bard_df %>%
  mutate(z_episodic = ((recall-mean(recall))/sd(recall)),
         z_semantic = ((know-mean(know))/sd(know)),
         z_realism = ((realrate-mean(realrate))/sd(realrate)),
         z_case_strength = ((rating-mean(rating))/sd(rating)))
#View(bard_z)
```

## Logistic Regression predicting BARD using memory measures and case strength rating

Like previous logistic regressions, this model uses the BOBYQA optimization method for 2e5 iterations. Running this model provides the following warning message: 

Warning: Model failed to converge with max|grad| = 0.0366524 (tol = 0.002, component 1)

```{r original model}
#specifiying model
b <-glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
#printing results in logits 
summary(b)
#printing table of results in odds ratios
tab_model(b, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))
```

## Checking singularity 

If the fit is singular (i.e. if the constrained parameters of the random effects are close to zero or < e10-6), this could cause misconvergence. 

```{r pressure, echo=FALSE}
#check singularity
tt <- getME(b,"theta")
ll <- getME(b,"lower")
min(tt[ll==0])
```
Singularity does not appear to be an issue here. 


# Restarting from previous fit 

```{r}
ss <- getME(b,c("theta","fixef"))
m_restart <- update(b,start=ss,control=glmerControl(optCtrl=list(maxfun=2e4)))
#printing results in logits 
summary(m_restart)
#printing table of results in odds ratios
tab_model(m_restart, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))

```

max|grad| is somewhat lower at 0.0355383 (compared to max|grad| = 0.0366524). However, restarting from previous fit still results in model convergence issues. 

# Trying a different optimization method

The default optimization method for glmer models is Nelder Mead. We will run the model using that default setting for 2e5 iterations. 

```{r}
#try Nelder Mead method? 
b_mead <-glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial)
summary(b_mead)
tab_model(b_mead, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))
```
max|grad| is lower at 0.0156244. Ideally, we'd like this value to be < 0.001 so convergence issues persist.

# Changing the tolerances for original model withh BOBYQA optimizer

resources: https://search.r-project.org/CRAN/refmans/lme4/html/lmerControl.html

https://cran.r-project.org/web/packages/lme4/vignettes/lmerperf.html 

```{r}
```


# Notes 

Setting a random seed so people can try exactly where its starting from different place

# Running with different random seeds

```{r}
set.seed(23)
#specifiying model
b_1 <-glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
#printing results in logits 
summary(b_1)
#printing table of results in odds ratios
tab_model(b_1, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))
```


```{r}
set.seed(47)
#specifiying model
b_1 <-glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
#printing results in logits 
summary(b_1)
#printing table of results in odds ratios
tab_model(b_1, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))
```

```{r}
set.seed(73)
#specifiying model
b_1 <-glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
#printing results in logits 
summary(b_1)
#printing table of results in odds ratios
tab_model(b_1, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))
```

```{r}
set.seed(99)
#specifiying model
b_1 <-glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
#printing results in logits 
summary(b_1)
#printing table of results in odds ratios
tab_model(b_1, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))
```

Notice that even with different random seeds, max|grad| is still 0.0366524. 

# Plotting residuals (of original model)

resource: https://www.geeksforgeeks.org/how-to-create-a-residual-plot-in-r/ 

```{r}
# get list of residuals  
res <- resid(b) 

# produce residual vs. fitted plot 
plot(fitted(b), res) 
  
# add a horizontal line at 0  
abline(0,0) 

# create Q-Q plot for residuals 
qqnorm(res) 
  
# add a straight diagonal line  
# to the plot 
qqline(res)

#distribution of residuals
plot(density(res))



#plotting residuals
plot(predict(b),residuals(b))
abline(h=0,lty=2,col="grey")
```
## Getting residuals from model that just contains case strength rating 

```{r}
#running model just using case strength to predict bard
c_bard <-glmer(bard ~ z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
#printing results in logits 
summary(c_bard)
#printing table of results in odds ratios
tab_model(c_bard, pred.labels = c("Intercept","Case Strength"), dv.labels = c("BARD"))

#extracting residuals from this model 
bard_z$case_s_residuals <-residuals(c_bard)

#check distribution of residuals
hist(bard_z$case_s_residuals)

```

# Modeling case strength residuals as function of memory measures 

```{r}

#linear regression of case strength residuals on memory
lin_resid <-lm(case_s_residuals ~ z_episodic + z_semantic + z_realism, data = bard_z)
summary(lin_resid)
tab_model(lin_resid, pred.labels = c("Intercept","Episodic", "Semantic", "Realism"), dv.labels = c("Case Strength Residual"))

#include random effects in linear regression - did the random effects already go away from previous glmer model?
# lin_r_mixed <-lmer(case_s_residuals ~ z_episodic + z_semantic + z_realism + (1|num_uid) + (1|scenario), data = bard_z)
# summary(lin_r_mixed)
# tab_model(lin_r_mixed, pred.labels = c("Intercept","Episodic", "Semantic", "Realism"), dv.labels = c("Case Strength Residual"))


lin_r <-residuals(lin_resid)

lin_pred <-fitted(lin_r)


#plotting fitted values against residuals 
res.linear <- residuals(lin_resid, type="response")
par(bg="white", cex=1.2)
plot(predict(lin_resid)[y==0], res.linear[y==0], las=1,
     xlab="Fitted values", ylab = "Residuals",
     col="red")
points(predict(lin_resid)[y==1], res.linear[y==1], col="blue")
abline(h = 0, lty = 2)

plot(case_s_residuals ~ z_episodic + z_semantic + z_realism, data= bard_z, las=1, ylim=c(-0.1, 1.3))



```

## Getting residuals from model that just contains memory ratings

```{r}
#running model just using case strength to predict bard
mem_bard <-glmer(bard ~ z_episodic + z_semantic + z_realism + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
#printing results in logits 
summary(mem_bard)
#printing table of results in odds ratios
tab_model(mem_bard, pred.labels = c("Intercept","Episodic", "Semantic", "Realism"), dv.labels = c("BARD"))

#extracting residuals from this model 
bard_z$memory_residuals <-residuals(mem_bard)

#check distribution of residuals
hist(bard_z$memory_residuals)
```


```{r}

```

# Modeling memory residuals as a function of case strength rating

```{r}
#specifying linear regression

lin_mem <- lm(memory_residuals ~ z_case_strength, data = bard_z)
summary(lin_mem)

tab_model(lin_mem, pred.labels = c("Intercept", "Case Strength Rating"), dv.labels = c("Memory Residuals"))

```

```{r}

```

# Steadily increasing the iterations of original glmer model

```{r}
#specifiying model
p_1 <-glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e2)))
#printing results in logits 
summary(p_1)
#printing table of results in odds ratios
tab_model(p_1, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))
```


```{r}
#specifiying model
p_2 <-glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e3)))
#printing results in logits 
summary(p_2)
#printing table of results in odds ratios
tab_model(p_2, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))
```


```{r}
#specifiying model
p_3 <-glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e4)))
#printing results in logits 
summary(p_3)
#printing table of results in odds ratios
tab_model(p_3, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))
```

Iterations: 20 million 

```{r}
#specifiying model
p_4 <-glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e7)))
#printing results in logits 
summary(p_4)
#printing table of results in odds ratios
tab_model(p_4, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))
```

Iterations: 2 billion

```{r}
#specifiying model
p_5 <-glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e9)))
#printing results in logits 
summary(p_5)
#printing table of results in odds ratios
tab_model(p_5, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))
```


```{r}
#specifiying model
p_6 <-glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e10)))
#printing results in logits 
summary(p_6)
#printing table of results in odds ratios
tab_model(p_6, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))
```

https://stackoverflow.com/questions/63179963/convergence-code-1-glmer-model-lme4-package 

```{r}
#fitting model with all available optimizers
allFit(b)
```
nlminbwrap yields singular fit; let's examine the results when fit with that optimizer

```{r}
b_nlim <-glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="nlminbwrap", optCtrl=list(maxfun=2e5)))
#printing results in logits 
summary(b_nlim)
#printing table of results in odds ratios
tab_model(b_nlim, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))
```

## Back to MCMC Land- using Rstanarm for estimation of original glmer model 

https://cran.rstudio.com/web/packages/rstanarm/

```{r}
#installing rstanarm 
install.packages("rstanarm")
library(rstanarm)

#update rstan
update.packages("rstan")

#check version of rstan
packageVersion("rstan")

#updating cmdstan
install_cmdstan()
```

```{r}
#running GLMER model using rstanarm

library(rstanarm)
library(rethinking)
#install.packages("modelsummary")
library(modelsummary)

bard_stan <- stan_glmer(bard ~ z_episodic + z_semantic + z_realism + z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial)

#viewing results
summary(bard_stan)

precis(bard_stan, prob = .95)

#alternative view
modelsummary(bard_stan, statistic = "conf.int")

#note which estimates have a confidence interval that includes zero 

#does sjPlot work with stan models? lets see
tab_model(bard_stan, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))

#TODO: print results of modelsummary and the sjPlot table to see for meeting

```


## Plottting case strength z scored against bard

```{r}

bard_z$preds = posterior_predict(bard_case, size = 2573, type="response")
```




```{r}
#stan model sjPlot

tab_model(bard_stan, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism", "Case Strength"), dv.labels = c("BARD"))
```

```{r}
## running glmer with only case strength
library(rstanarm)
bard_case <- stan_glmer(bard ~ z_case_strength + (1|num_uid) + (1|scenario), data = bard_z, family = binomial)

#viewing results
summary(bard_case)



#alternative view
modelsummary(bard_case, statistic = "conf.int")

#note which estimates have a confidence interval that includes zero 

#printing odds ratios for results
tab_model(bard_case, pred.labels = c("Intercept", "Case Strength"), dv.labels = c("BARD"))

```

# GLMER just memory effects on BARD

```{r}
#model
bard_memory_stan <- stan_glmer(bard ~ z_episodic + z_semantic + z_realism + (1|num_uid) + (1|scenario), data = bard_z, family = binomial)

#viewing results
summary(bard_memory_stan)

modelsummary(bard_memory_stan, statistic = "conf.int")

#sjplot
tab_model(bard_memory_stan, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism"), dv.labels = c("BARD"))
```

# Rerun LMER of memory effects on case strength 

This may require us to revisit the variance partitioning as well depending on the estimates. 


```{r}
#model
case_stan_mem <- stan_glmer(z_case_strength ~ z_episodic + z_semantic + z_realism + (1|num_uid) + (1|scenario), data = bard_z, family = gaussian)

summary(case_stan_mem)

#sjplot
tab_model(case_stan_mem, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism"), dv.labels = c("Case Strength"))
```

# Variance Partitioning LMER

```{r}
library(partR2)
llm_stan <- partR2(case_stan_mem, partvars = c("z_episodic", "z_semantic", "z_realism"), nboot=100)
print(llm_var)
```

# Plotting logistic curve - case strength z scores predicting BARD z scores

```{r}



# Create a new data frame with a range of predictor values
plt_df <- expand.grid(z_cs = seq(min(bard_z$z_case_strength), max(bard_z$z_case_strength), length.out = 100))


#probably want to sample from posterior first and then grab those predicted probabilitues from samples

install.packages("posterior")
library(posterior)

samples <- as_draws_df(bard_case)

View(samples)

#View(plt_df2)

#TODO; predict probabialities with dataframe that has all information in it 
#in this case, we'll only need num_uid, Scenario, and z_case_strength and BARD

#if this doesn't work, we plot based on the glmer regression using lme4 with the z scored predictors 


# Predict probabilities
plt_df$prob <- extract(samples, plt_df2, type = "response")

plt_4 <- ggplot(b_df) + geom_smooth(aes(x=rating10, y=pred), color = "slateblue")  + 
  geom_point(aes(x=rating10, y = bard), color = "plum3") +
  xlab("Case Strength (points)") +
  ylab("Probability of Voting Guilty") 
plt_4


#curves for each of the memory measures as well
```



Here the easiest decision might be to use lme4 for the case strength model but then all logistic regressions use stan since thats not compatible; 

# Comment Dump

```{r}
# #specifying model 
# b_resid <- glmer(bard ~ z_episodic + z_semantic + z_realism + case_s_residuals + (1|num_uid) + (1|scenario), data = bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))



# #specifying model 
# b_mem <-glmer(bard ~ z_episodic + z_semantic + z_realism + (1|num_uid) + (1|scenario), data = df_bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
# #printing results
# summary(b_mem)
# tab_model(b_mem, pred.labels = c("Intercept", "Episodic Memory", "Semantic Memory", "Perceived Realism"), dv.labels = c("BARD"))
# 
# #extracting residuals from this model 
# bard_z$mem_resid <-residuals(b_mem)
# 
# #check distribution of residuals
# hist(bard_z$mem_resid)

# #specifying model 
# b_epi <-glmer(bard ~ z_episodic + (1|num_uid) + (1|scenario), data = df_bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
# 
# summary(b_epi)
# 
# bard_z$episodic_resid <- residuals(b_epi)
# 
# #reisudals for semantic
# b_sem <-glmer(bard ~ z_semantic + (1|num_uid) + (1|scenario), data = df_bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
# 
# summary(b_sem)
# 
# bard_z$semantic_resid <- residuals(b_sem)
# 
# #residuals for realism 
# b_real <-glmer(bard ~ z_realism + (1|num_uid) + (1|scenario), data = df_bard_z, family = binomial, control=glmerControl(optimizer="bobyqa", optCtrl=list(maxfun=2e5)))
# 
# summary(b_real)
# 
# bard_z$realism_resid <- residuals(b_real)
# 
# #distribution of residuals for each memory rating
# hist(bard_z$episodic_resid)
# hist(bard_z$semantic_resid)
# hist(bard_z$realism_resid)


#predict BARD using memory measures in logistic regression w/ random effects

#extract residuals of above model 

#predict memory residuals using case strength rating w/ linear regression (no random effects)
```

